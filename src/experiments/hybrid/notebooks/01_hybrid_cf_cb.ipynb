{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Hybrid - merged CF and CB__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%watermark -v -n -m -p numpy,pandas,recmetrics,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import recmetrics\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "sys.path.append('../../../../')\n",
    "from src.settings import DATA_DIR, PROJECT_DIR, RESULT_DIR\n",
    "\n",
    "CURRENT_PATH = os.path.abspath(os.path.join(os.pardir))\n",
    "print(CURRENT_PATH)\n",
    "print(DATA_DIR)\n",
    "print(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from src.data_processing.visualization.plot_utils import plot_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_HYBRID_DIR = Path(PROJECT_DIR) / 'reports' / 'figures' / 'sec5_hybrid'\n",
    "Path(REPORTS_HYBRID_DIR).mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_HYBRID_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MOVIES_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'movies_test_1k_users.csv'\n",
    "TEST_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_test_1k_users.csv'\n",
    "TRAIN_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_train_1k_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = pd.read_csv(TEST_RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANI_RATE = 3.5\n",
    "relevant_ratings = test_ratings.query(f'rating >={RELEVANI_RATE}')\n",
    "relevant_ratings = relevant_ratings.drop(columns=['timestamp'])\n",
    "relevant_ratings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_cb_user_recommendation(df: pd.DataFrame, cb_model, user_id: int, pred_rate_col: str, rate_threshold=3.5, top_n=10):\n",
    "    df = df[df.userId == user_id]\n",
    "    df = df[df[pred_rate_col] >= rate_threshold]\n",
    "    liked_film_indices = df.movieId.values.tolist()\n",
    "    cb_recs = cb_model.get_recommendations_for_user(user_id, top=13623)\n",
    "    cb_recs = [(movie_id, cosine) for (title, movie_id, cosine) in cb_recs if movie_id in liked_film_indices]\n",
    "    \n",
    "    recommended_items = cb_recs[0:top_n]\n",
    "    recommended_items = [movie_id for (movie_id, cosine) in recommended_items]\n",
    "    \n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = Path(RESULT_DIR) / 'logs'\n",
    "MODEL_DIR = Path(RESULT_DIR) / 'models'\n",
    "CHECKPOINT_DIR = Path(RESULT_DIR) / 'checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __FunkSVD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNK_MODEL_PATH = Path(MODEL_DIR) / 'funk_svd' / 'compare_common_split' / '2020-06-01_14-44_FunkSVD-explicit-test-data.pkl'\n",
    "FUNK_LOGS_DIR = Path(RESULT_DIR) / 'logs' / 'funk_svd' / 'compare_common_split'\n",
    "FUNK_RESULTS_PATH = Path(FUNK_LOGS_DIR) / '2020-06-01_14-44_FunkSVD_test_data_spearman.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FUNK_RESULTS_PATH, 'rb') as f:\n",
    "    funk_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_true_rates = funk_results['true_rates']\n",
    "funk_pred_rates = funk_results['pred_rates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Tag model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.content_based.tag_model import TagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_model = TagModel(ratings_path=TRAIN_RATINGS_PATH, movies_path=TEST_MOVIES_PATH, rate_threshold=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_model.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Text model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.content_based.text_model import TextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TextModel(ratings_path=TRAIN_RATINGS_PATH, movies_path=TEST_MOVIES_PATH, rate_threshold=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Compare metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = relevant_ratings.copy().groupby('userId')['movieId'].agg(actual=(lambda x: list(set(x)))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = test_ratings.assign(funk_pred_rate = funk_pred_rates) \n",
    "test_ratings.drop(columns=['timestamp'], inplace=True)\n",
    "test_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Funk with tag model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "funk_tag_recs = []\n",
    "for user in tqdm(test_df_format.userId.values, desc='Getting recommendations', total=len(test_df_format.index)):\n",
    "    funk_pred = get_cf_cb_user_recommendation(test_ratings, tag_model, user, pred_rate_col='funk_pred_rate', top_n=TOP_N)\n",
    "    funk_tag_recs.append(funk_pred)\n",
    "        \n",
    "test_df_format['funk_tag_pred'] = funk_tag_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Funk with text model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "funk_text_recs = []\n",
    "for user in tqdm(test_df_format.userId.values, desc='Getting recommendations', total=len(test_df_format.index)):\n",
    "    funk_pred = get_cf_cb_user_recommendation(test_ratings, text_model, user, pred_rate_col='funk_pred_rate', top_n=TOP_N)\n",
    "    funk_text_recs.append(funk_pred)\n",
    "        \n",
    "test_df_format['funk_text_pred'] = funk_text_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.to_csv(Path(DATA_DIR) / 'datasets' / 'hybrid_compare.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_df_format.actual.values.tolist()\n",
    "funk_tag_predictions = test_df_format.funk_tag_pred.values.tolist()\n",
    "funk_text_predictions = test_df_format.funk_text_pred.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load compare dataframe if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = pd.read_csv(Path(DATA_DIR) / 'datasets' / 'hybrid_compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "actual = test_df_format.actual.values.tolist()\n",
    "funk_tag_predictions = test_df_format.funk_tag_pred.values.tolist()\n",
    "funk_text_predictions = test_df_format.funk_text_pred.values.tolist()\n",
    "\n",
    "types = []\n",
    "for pred_list in funk_tag_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])\n",
    "funk_tag_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in funk_text_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)]) \n",
    "funk_text_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in actual:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])  \n",
    "actual = types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_tag_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __MAP@K precision__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metrics import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_tag_mapk = []\n",
    "for K in np.arange(1, TOP_N+1):\n",
    "    funk_tag_mapk.extend([mapk(actual, funk_tag_predictions, k=K)])\n",
    "funk_tag_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_text_mapk = []\n",
    "for K in np.arange(1, TOP_N+1):\n",
    "    funk_text_mapk.extend([mapk(actual, funk_text_predictions, k=K)])\n",
    "funk_text_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk_df = pd.DataFrame(np.column_stack([funk_tag_mapk, funk_text_mapk]), range(1,TOP_N+1), \n",
    "                       columns=['Hybrid FunkSVD-Tag', 'Hybrid FunkSVD-Text'])\n",
    "\n",
    "ax = plot_line(mapk_df, title='Mean Average Precision at K (MAP@K) comparison', ylabel='MAP@K', xlabel='K', \n",
    "               linewidth=2.0, palette='tab10')\n",
    "plt.xticks(range(1,TOP_N+1))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_HYBRID_DIR, 'hybrid_map_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __MAR@K mean average recall at K__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_tag_mark = []\n",
    "for K in np.arange(1, 11):\n",
    "    funk_tag_mark.extend([recmetrics.mark(actual, funk_tag_predictions, k=K)])\n",
    "funk_tag_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_text_mark = []\n",
    "for K in np.arange(1, 11):\n",
    "    funk_text_mark.extend([recmetrics.mark(actual, funk_text_predictions, k=K)])\n",
    "funk_text_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_df = pd.DataFrame(np.column_stack([funk_tag_mark, funk_text_mark]), range(1,TOP_N+1), \n",
    "                       columns=['Hybrid FunkSVD-Tag', 'Hybrid FunkSVD-Text'])\n",
    "\n",
    "ax = plot_line(mark_df, title='Mean Average Recall at K (MAR@K) comparison', ylabel='MAR@K', xlabel='K', \n",
    "               linewidth=2.0, palette='tab10')\n",
    "plt.xticks(range(1,TOP_N+1))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_HYBRID_DIR, 'hybrid_mar_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Coverage__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movies = pd.read_csv(TEST_MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERAGE_TOP = 10\n",
    "\n",
    "all_movies = test_movies.movieId.unique().tolist()\n",
    "\n",
    "funk_tag_coverage = []\n",
    "for sublist in funk_tag_predictions:\n",
    "    funk_tag_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "    \n",
    "funk_text_coverage = []\n",
    "for sublist in funk_text_predictions:\n",
    "    funk_text_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "\n",
    "funk_tag_coverage = recmetrics.prediction_coverage(funk_tag_coverage, all_movies)\n",
    "funk_text_coverage = recmetrics.prediction_coverage(funk_text_coverage, all_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax = sns.barplot(x=['Hybrid FunkSVD-Tag', 'Hybrid FunkSVD-Text'], \n",
    "                     y=[funk_tag_coverage, funk_text_coverage], palette='tab10')\n",
    "    ax.set_title(f'Test movies coverage in top {COVERAGE_TOP} recommendations', fontsize=12.0)\n",
    "    ax.set_ylabel('coverage [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_HYBRID_DIR, f'hybrid_coverage_for_{COVERAGE_TOP}_top.png'), bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "rec-sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
