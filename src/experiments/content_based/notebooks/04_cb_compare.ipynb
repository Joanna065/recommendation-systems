{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CB models comparison__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%watermark -v -n -m -p numpy,pandas,recmetrics,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import recmetrics\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "sys.path.append('../../../../')\n",
    "from src.settings import DATA_DIR, PROJECT_DIR, RESULT_DIR\n",
    "\n",
    "CURRENT_PATH = os.path.abspath(os.path.join(os.pardir))\n",
    "print(CURRENT_PATH)\n",
    "print(DATA_DIR)\n",
    "print(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from src.data_processing.visualization.plot_utils import plot_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_CB_DIR = Path(PROJECT_DIR) / 'reports' / 'figures' / 'sec3_cb'\n",
    "Path(REPORTS_CB_DIR).mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_CB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MOVIES_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'movies_test_1k_users.csv'\n",
    "TEST_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_test_1k_users.csv'\n",
    "TRAIN_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_train_1k_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = pd.read_csv(TEST_RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANI_RATE = 3.5\n",
    "relevant_ratings = test_ratings.query(f'rating >={RELEVANI_RATE}')\n",
    "relevant_ratings = relevant_ratings.drop(columns=['timestamp'])\n",
    "relevant_ratings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### __Train models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.content_based.tag_model import TagModel\n",
    "\n",
    "tag_model = TagModel(ratings_path=TRAIN_RATINGS_PATH, movies_path=TEST_MOVIES_PATH, rate_threshold=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_model.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.content_based.text_model import TextModel\n",
    "\n",
    "text_model = TextModel(ratings_path=TRAIN_RATINGS_PATH, movies_path=TEST_MOVIES_PATH, rate_threshold=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### __Compare metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = relevant_ratings.copy().groupby('userId')['movieId'].agg(actual=(lambda x: list(set(x)))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendation(model, user_id: int, top_n=10):\n",
    "    cb_recs = model.get_recommendations_for_user(user_id, top=top_n)\n",
    "    cb_recs = [movie_id for (title, movie_id, cosine) in cb_recs]\n",
    "    \n",
    "    return cb_recs[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tag_recs = []\n",
    "for user in tqdm(test_df_format.userId.values, desc='Getting recommendations', total=len(test_df_format.index)):\n",
    "    funk_pred = get_user_recommendation(tag_model, user, top_n=TOP_N)\n",
    "    tag_recs.append(funk_pred)\n",
    "        \n",
    "test_df_format['tag_pred'] = tag_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "text_recs = []\n",
    "for user in tqdm(test_df_format.userId.values, desc='Getting recommendations', total=len(test_df_format.index)):\n",
    "    funk_pred = get_user_recommendation(text_model, user, top_n=TOP_N)\n",
    "    text_recs.append(funk_pred)\n",
    "        \n",
    "test_df_format['text_pred'] = text_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.to_csv(Path(DATA_DIR) / 'datasets' / 'tag_text_compare.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_df_format.actual.values.tolist()\n",
    "tag_predictions = test_df_format.tag_pred.values.tolist()\n",
    "text_predictions = test_df_format.text_pred.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "##### Read if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = pd.read_csv(Path(DATA_DIR) / 'datasets' / 'tag_text_compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "actual = test_df_format.actual.values.tolist()\n",
    "tag_predictions = test_df_format.tag_pred.values.tolist()\n",
    "text_predictions = test_df_format.text_pred.values.tolist()\n",
    "\n",
    "types = []\n",
    "for pred_list in tag_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])\n",
    "tag_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in text_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)]) \n",
    "text_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in actual:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])  \n",
    "actual = types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __MAP@K precision__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metrics import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapk = []\n",
    "for K in np.arange(1, TOP_N+1):\n",
    "    tag_mapk.extend([mapk(actual, tag_predictions, k=K)])\n",
    "tag_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mapk = []\n",
    "for K in np.arange(1, TOP_N+1):\n",
    "    text_mapk.extend([mapk(actual, text_predictions, k=K)])\n",
    "text_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk_df = pd.DataFrame(np.column_stack([tag_mapk, text_mapk]), range(1,TOP_N+1), columns=['Tag model', 'Text model'])\n",
    "\n",
    "ax = plot_line(mapk_df, title='Mean Average Precision at K (MAP@K) comparison', ylabel='MAP@K', xlabel='K', \n",
    "               linewidth=3.0, palette='tab10')\n",
    "plt.xticks(range(1,11))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CB_DIR, 'cb_map_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### __MAR@K recall__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mark = []\n",
    "for K in np.arange(1, 11):\n",
    "    tag_mark.extend([recmetrics.mark(actual, tag_predictions, k=K)])\n",
    "tag_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mark = []\n",
    "for K in np.arange(1, 11):\n",
    "    text_mark.extend([recmetrics.mark(actual, text_predictions, k=K)])\n",
    "text_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_df = pd.DataFrame(np.column_stack([tag_mark, text_mark]), range(1,TOP_N+1), columns=['Tag model', 'Text model'])\n",
    "\n",
    "ax = plot_line(mark_df, title='Mean Average Recall at K (MAR@K) comparison', ylabel='MAR@K', xlabel='K', \n",
    "               linewidth=3.0, palette='tab10')\n",
    "plt.xticks(range(1,11))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CB_DIR, 'cb_mar_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### __Coverage__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movies = pd.read_csv(TEST_MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERAGE_TOP = 10\n",
    "\n",
    "all_movies = test_movies.movieId.unique().tolist()\n",
    "tag_coverage = []\n",
    "for sublist in tag_predictions:\n",
    "    tag_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "    \n",
    "text_coverage = []\n",
    "for sublist in text_predictions:\n",
    "    text_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "\n",
    "tag_coverage = recmetrics.prediction_coverage(tag_coverage, all_movies)\n",
    "text_coverage = recmetrics.prediction_coverage(text_coverage, all_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax = sns.barplot(x=['Tag model', 'Text model'], y=[tag_coverage, text_coverage], palette='tab10')\n",
    "    ax.set_title(f'Test movies coverage in top {COVERAGE_TOP} recommendations', fontsize=12.0)\n",
    "    ax.set_ylabel('coverage [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CB_DIR, f'cb_coverage_for_{COVERAGE_TOP}_top.png'), bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "rec-sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
