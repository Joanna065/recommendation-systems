{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CF models comparison__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%watermark -v -n -m -p numpy,pandas,recmetrics,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('../../../../')\n",
    "from src.settings import DATA_DIR, PROJECT_DIR, RESULT_DIR\n",
    "\n",
    "CURRENT_PATH = os.path.abspath(os.path.join(os.pardir))\n",
    "print(CURRENT_PATH)\n",
    "print(DATA_DIR)\n",
    "print(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import recmetrics\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from src.data_processing.visualization.plot_utils import plot_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_CF_DIR = Path(PROJECT_DIR) / 'reports' / 'figures' / 'sec4_cf'\n",
    "Path(REPORTS_CF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_CF_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MOVIES_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'movies_test_1k_users.csv'\n",
    "TEST_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_test_1k_users.csv'\n",
    "TRAIN_RATINGS_PATH = Path(DATA_DIR) / 'datasets' / 'compare_split' / 'ratings_train_1k_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = pd.read_csv(TEST_RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANI_RATE = 3.5\n",
    "relevant_ratings = test_ratings.query(f'rating >={RELEVANI_RATE}')\n",
    "relevant_ratings = relevant_ratings.drop(columns=['timestamp'])\n",
    "relevant_ratings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendation(df, user_id, sort_col, top_n=10):\n",
    "    df = df[df.userId == user_id]\n",
    "    df = df.sort_values(by=[sort_col], ascending=False)\n",
    "    recommended_items = df.movieId.values[0:top_n]\n",
    "    \n",
    "    return recommended_items.tolist()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_user_recommendation(model, user_id: int, top_n=10):\n",
    "    recs = model.get_recommendations_for_user(user_id=user_id, top=top_n)\n",
    "    recs_movies = [movie_id for (title, movie_id, cosine) in recs]\n",
    "    \n",
    "    return recs_movies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Load models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = Path(RESULT_DIR) / 'logs'\n",
    "MODEL_DIR = Path(RESULT_DIR) / 'models'\n",
    "CHECKPOINT_DIR = Path(RESULT_DIR) / 'checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __NeuMf (301 epochs)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '2020-06-02_17-25_NeuFM_compare_split'\n",
    "NEUMF_RESULTS_PATH = Path(LOGS_DIR) / 'neural_mf' / MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(NEUMF_RESULTS_PATH) / 'spearman.pkl', 'rb') as f:\n",
    "    neumf_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_true_rates = neumf_results['true_rates']\n",
    "neumf_pred_rates = np.clip(neumf_results['pred_rates'].flatten(), 0.5, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __FunkSVD (13 epochs)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNK_MODEL_PATH = Path(MODEL_DIR) / 'funk_svd' / 'compare_common_split' / '2020-06-01_14-44_FunkSVD-explicit-test-data.pkl'\n",
    "FUNK_LOGS_DIR = Path(RESULT_DIR) / 'logs' / 'funk_svd' / 'compare_common_split'\n",
    "FUNK_RESULTS_PATH = Path(FUNK_LOGS_DIR) / '2020-06-01_14-44_FunkSVD_test_data_spearman.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FUNK_RESULTS_PATH, 'rb') as f:\n",
    "    funk_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_true_rates = funk_results['true_rates']\n",
    "funk_pred_rates = funk_results['pred_rates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __KNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.collaborative_filtering.knn import KNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNNModel(ratings_path=TRAIN_RATINGS_PATH, movies_path=TEST_MOVIES_PATH, ratings_threshold=3.5, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.get_recommendations_for_user(user_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.get_recommendations_for_movie('The Shawshank Redemption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.get_recommendations_for_movie('Iron Man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Compare metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 100\n",
    "METRIC_TOP_N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format test dataframe with relevant items to have __list__ of actual liked films (rate >= 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = relevant_ratings.copy().groupby('userId')['movieId'].agg(actual=(lambda x: list(set(x)))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings = test_ratings.assign(funk_pred_rate = funk_pred_rates) \n",
    "test_ratings = test_ratings.assign(neumf_pred_rate = neumf_pred_rates) \n",
    "test_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def get_spearman_for_knn(df: pd.DataFrame, model):\n",
    "    df = df.sort_values(by=['userId'])\n",
    "    true_rates = df.rating.values.tolist()\n",
    "    \n",
    "    unique_users = np.unique(df['userId'].values)\n",
    "    \n",
    "    pred_distance = []    \n",
    "    true_rates = []\n",
    "    for user_id in tqdm(unique_users, desc='Getting recommendations for users', total=len(unique_users)):\n",
    "        knn_pred = model.get_recommendations_for_user(user_id, top=13623)\n",
    "        user_df = df[df['userId'] == user_id]\n",
    "        liked_film_indices = user_df.movieId.values.tolist()\n",
    "        knn_pred = [(movie_id, cosine) for (title, movie_id, cosine) in knn_pred if movie_id in liked_film_indices]\n",
    "        pred_movie_indices = [t[0] for t in knn_pred]\n",
    "        \n",
    "        rates = []\n",
    "        for movie_id in pred_movie_indices:\n",
    "            rate = user_df[user_df.movieId == movie_id].rating.values.tolist()\n",
    "            rates.extend(rate)\n",
    "        \n",
    "        pred_distance.extend(knn_pred)\n",
    "        true_rates.extend(rates)\n",
    "        \n",
    "    spearman, p_val = spearmanr(true_rates, pred_distance)\n",
    "\n",
    "    return spearman, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman, p_val = get_spearman_for_knn(test_ratings, knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add list of recommendations of models from all test data films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_recs = []\n",
    "for user in test_df_format.userId.values:\n",
    "    funk_pred = get_user_recommendation(test_ratings, user, sort_col='funk_pred_rate', top_n=TOP_N)\n",
    "    funk_recs.append(funk_pred)\n",
    "        \n",
    "test_df_format['funk_pred'] = funk_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_recs = []\n",
    "for user in test_df_format.userId.values:\n",
    "    neumf_pred = get_user_recommendation(test_ratings, user, sort_col='neumf_pred_rate', top_n=TOP_N)\n",
    "    neumf_recs.append(neumf_pred)\n",
    "        \n",
    "test_df_format['neumf_pred'] = neumf_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "knn_recs = []\n",
    "for user in tqdm(test_df_format.userId.values, desc='Getting recommendations', total=len(test_df_format.index)):\n",
    "    knn_pred = get_knn_user_recommendation(knn_model, user, top_n=TOP_N)\n",
    "    knn_recs.append(knn_pred)\n",
    "        \n",
    "test_df_format['knn_pred'] = knn_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format.to_csv(Path(DATA_DIR) / 'datasets' / 'cf_models_compare.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_df_format.actual.values.tolist()\n",
    "funk_predictions = test_df_format.funk_pred.values.tolist()\n",
    "neumf_predictions = test_df_format.neumf_pred.values.tolist()\n",
    "knn_predictions = test_df_format.knn_pred.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Load compare dataframe if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_format = pd.read_csv(Path(DATA_DIR) / 'datasets' / 'cf_models_compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "actual = test_df_format.actual.values.tolist()\n",
    "funk_predictions = test_df_format.funk_pred.values.tolist()\n",
    "neumf_predictions = test_df_format.neumf_pred.values.tolist()\n",
    "knn_predictions = test_df_format.knn_pred.values.tolist()\n",
    "\n",
    "types = []\n",
    "for pred_list in funk_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])\n",
    "funk_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in neumf_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)]) \n",
    "neumf_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in knn_predictions:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)]) \n",
    "knn_predictions = types\n",
    "\n",
    "types = []\n",
    "for pred_list in actual:\n",
    "    types.append([int(x) for x in ast.literal_eval(pred_list)])  \n",
    "actual = types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### __MAP@K mean average precision at K__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metrics import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_mapk = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    neumf_mapk.extend([mapk(actual, neumf_predictions[0:METRIC_TOP_N], k=K)])\n",
    "neumf_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_mapk = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    funk_mapk.extend([mapk(actual, funk_predictions[0:METRIC_TOP_N], k=K)])\n",
    "funk_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mapk = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    knn_mapk.extend([mapk(actual, knn_predictions[0:METRIC_TOP_N], k=K)])\n",
    "knn_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk_df = pd.DataFrame(np.column_stack([knn_mapk, funk_mapk, neumf_mapk]), range(1,METRIC_TOP_N+1), columns=['KNN', 'FunkSVD', 'NeuMF'])\n",
    "\n",
    "ax = plot_line(mapk_df, title='Mean Average Precision at K (MAP@K) comparison', ylabel='MAP@K', xlabel='K', \n",
    "               linewidth=2.0, palette='tab10')\n",
    "plt.xticks(range(1,METRIC_TOP_N+1))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CF_DIR, 'cf_map_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### __MAR@K mean average recall at K__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_mark = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    neumf_mark.extend([recmetrics.mark(actual, neumf_predictions[0:METRIC_TOP_N], k=K)])\n",
    "neumf_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_mark = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    funk_mark.extend([recmetrics.mark(actual, funk_predictions[0:METRIC_TOP_N], k=K)])\n",
    "funk_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mark = []\n",
    "for K in np.arange(1, METRIC_TOP_N+1):\n",
    "    knn_mark.extend([recmetrics.mark(actual, knn_predictions[0:METRIC_TOP_N], k=K)])\n",
    "knn_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_df = pd.DataFrame(np.column_stack([knn_mark, funk_mark, neumf_mark]), range(1,METRIC_TOP_N+1), columns=['KNN', 'FunkSVD', 'NeuMF'])\n",
    "\n",
    "ax = plot_line(mark_df, title='Mean Average Recall at K (MAR@K) comparison', ylabel='MAR@K', xlabel='K', \n",
    "               linewidth=2.0, palette='tab10')\n",
    "plt.xticks(range(1,METRIC_TOP_N+1))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CF_DIR, 'cf_mar_at_k.png'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### __Coverage__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movies = pd.read_csv(TEST_MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERAGE_TOP = 10\n",
    "\n",
    "all_movies = test_movies.movieId.unique().tolist()\n",
    "\n",
    "funk_coverage = []\n",
    "for sublist in funk_predictions:\n",
    "    funk_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "    \n",
    "neumf_coverage = []\n",
    "for sublist in neumf_predictions:\n",
    "    neumf_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "\n",
    "knn_coverage = []\n",
    "for sublist in knn_predictions:\n",
    "    knn_coverage.append(sublist[0:COVERAGE_TOP])\n",
    "    \n",
    "funk_coverage = recmetrics.prediction_coverage(funk_coverage, all_movies)\n",
    "neumf_coverage = recmetrics.prediction_coverage(neumf_coverage, all_movies)\n",
    "knn_coverage = recmetrics.prediction_coverage(knn_coverage, all_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax = sns.barplot(x=['KNN', 'FunkSVD', 'NeuMF'], \n",
    "                     y=[knn_coverage, funk_coverage, neumf_coverage], palette='tab10')\n",
    "    ax.set_title(f'Test movies coverage in top {COVERAGE_TOP} recommendations', fontsize=12.0)\n",
    "    ax.set_ylabel('coverage [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(REPORTS_CF_DIR, f'cf_coverage_for_{COVERAGE_TOP}_top.png'), bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "rec-sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
